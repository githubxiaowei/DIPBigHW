{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 导入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type: <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      "shape: (5994, 224, 224, 3) (5994,)\n",
      "size: 902264832 5994\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "from tensorflow.contrib.slim.nets import vgg\n",
    "\n",
    "IMAGE_SIZE = vgg.vgg_16.default_image_size\n",
    "\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "def input_data(npz_file):\n",
    "    if os.path.exists(npz_file) :\n",
    "        bird_data = np.load(npz_file)\n",
    "        return bird_data['train_img'],bird_data['test_img'],bird_data['train_label'],bird_data['test_label']\n",
    "    else:      \n",
    "        data_path = os.path.join('../../..','data','CUB_200_2011')\n",
    "        print(os.listdir(data_path))\n",
    "\n",
    "        train_test_split_file = os.path.join(data_path,'train_test_split.txt')\n",
    "        with open(train_test_split_file,'r') as file:\n",
    "            train_test_split = np.array([i.split()[1] for i in file.readlines()]).astype('bool')\n",
    "        print(train_test_split,train_test_split.size)\n",
    "\n",
    "        img_paths_file = os.path.join(data_path,'images.txt')\n",
    "        with open(img_paths_file,'r') as file:\n",
    "            img_paths = [i.split()[1] for i in file.readlines()]\n",
    "        print(img_paths[:1],len(img_paths))\n",
    "\n",
    "        img_labels_file = os.path.join(data_path,'image_class_labels.txt')\n",
    "        with open(img_labels_file,'r') as file:\n",
    "            img_labels = np.array([i.split()[1] for i in file.readlines()]).astype('int')\n",
    "        print(img_labels,len(img_labels))\n",
    "\n",
    "        img_dir = os.path.join(data_path,'images')\n",
    "\n",
    "        img_paths_train = [os.path.join(img_dir,os.path.sep.join(path.split('/'))) for i,path in enumerate(img_paths) if train_test_split[i]]\n",
    "        print(img_paths_train[:1],len(img_paths_train))\n",
    "        img_paths_test = [os.path.join(img_dir,os.path.sep.join(path.split('/'))) for i,path in enumerate(img_paths) if not train_test_split[i]]\n",
    "        print(img_paths_test[:1],len(img_paths_test))\n",
    "\n",
    "        train_img = np.array([cv2.resize(cv2.imread(i),(224,224)) for i in img_paths_train])\n",
    "        test_img = np.array([cv2.resize(cv2.imread(i),(224,224)) for i in img_paths_test])\n",
    "        train_label = np.array([l for i,l in enumerate(img_labels) if train_test_split[i] ])\n",
    "        test_label = np.array([l for i,l in enumerate(img_labels) if not train_test_split[i]])\n",
    "        print(train_label,train_label.size)\n",
    "        print(test_label,test_label.size)\n",
    "\n",
    "        np.savez(npz_file,train_img=train_img,test_img=test_img,train_label=train_label,test_label=test_label)\n",
    "        return train_img,test_img,train_label,test_label\n",
    "    \n",
    "x_train,x_test,y_train,y_test = input_data('bird_data_224.npz')\n",
    "\n",
    "\n",
    "num_classes = 200\n",
    "\n",
    "# 数据预处理，把 0-255的灰度值转成 0-1 之间的浮点数\n",
    "x_train = x_train.astype('float32')/255\n",
    "x_test = x_test.astype('float32')/255\n",
    "y_train, y_test = np.array(y_train)-1, np.array(y_test)-1\n",
    "\n",
    "\n",
    "\n",
    "print('type:',type(x_train),type(y_train))\n",
    "print('shape:',x_train.shape,y_train.shape)\n",
    "print('size:',x_train.size,y_train.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 批量数据 生成器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2, 3, 4]]\n",
      "[[5, 6, 7, 8]]\n",
      "[[9]]\n"
     ]
    }
   ],
   "source": [
    "# 遍历一遍数据集，不打乱\n",
    "def batch_generator(data, batch_size):\n",
    "    batch_count = 0 \n",
    "    while batch_count * batch_size < len(data[0]):\n",
    "        start = batch_count * batch_size\n",
    "        end = np.min([start + batch_size,len(data[0])])\n",
    "        batch_count += 1\n",
    "        yield [d[start:end] for d in data]\n",
    "\n",
    "# 测试\n",
    "for batch in batch_generator([[1,2,3,4,5,6,7,8,9]],4):\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算数据集特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vgg_16/conv1/conv1_1\n",
      "vgg_16/conv1/conv1_2\n",
      "vgg_16/pool1\n",
      "vgg_16/conv2/conv2_1\n",
      "vgg_16/conv2/conv2_2\n",
      "vgg_16/pool2\n",
      "vgg_16/conv3/conv3_1\n",
      "vgg_16/conv3/conv3_2\n",
      "vgg_16/conv3/conv3_3\n",
      "vgg_16/pool3\n",
      "vgg_16/conv4/conv4_1\n",
      "vgg_16/conv4/conv4_2\n",
      "vgg_16/conv4/conv4_3\n",
      "vgg_16/pool4\n",
      "vgg_16/conv5/conv5_1\n",
      "vgg_16/conv5/conv5_2\n",
      "vgg_16/conv5/conv5_3\n",
      "vgg_16/pool5\n",
      "vgg_16/fc6\n",
      "vgg_16/fc7\n",
      "vgg_16/fc8\n",
      "INFO:tensorflow:Restoring parameters from ./vgg_16_2016_08_28/slim_fine_tune\\birds_fine_tune.ckpt-20\n",
      "加载上次训练保存后的模型！\n",
      "batch_size: 128 acc: 0.453 correct_num: 58.0\n",
      "batch_size: 128 acc: 0.711 correct_num: 91.0\n",
      "batch_size: 128 acc: 0.602 correct_num: 77.0\n",
      "batch_size: 128 acc: 0.688 correct_num: 88.0\n",
      "batch_size: 128 acc: 0.562 correct_num: 72.0\n",
      "batch_size: 128 acc: 0.414 correct_num: 53.0\n",
      "batch_size: 128 acc: 0.383 correct_num: 49.0\n",
      "batch_size: 128 acc: 0.594 correct_num: 76.0\n",
      "batch_size: 128 acc: 0.484 correct_num: 62.0\n",
      "batch_size: 128 acc: 0.586 correct_num: 75.0\n",
      "batch_size: 128 acc: 0.547 correct_num: 70.0\n",
      "batch_size: 128 acc: 0.773 correct_num: 99.0\n",
      "batch_size: 128 acc: 0.586 correct_num: 75.0\n",
      "batch_size: 128 acc: 0.359 correct_num: 46.0\n",
      "batch_size: 128 acc: 0.523 correct_num: 67.0\n",
      "batch_size: 128 acc: 0.445 correct_num: 57.0\n",
      "batch_size: 128 acc: 0.797 correct_num: 102.0\n",
      "batch_size: 128 acc: 0.625 correct_num: 80.0\n",
      "batch_size: 128 acc: 0.656 correct_num: 84.0\n",
      "batch_size: 128 acc: 0.703 correct_num: 90.0\n",
      "batch_size: 128 acc: 0.594 correct_num: 76.0\n",
      "batch_size: 128 acc: 0.586 correct_num: 75.0\n",
      "batch_size: 128 acc: 0.453 correct_num: 58.0\n",
      "batch_size: 128 acc: 0.562 correct_num: 72.0\n",
      "batch_size: 128 acc: 0.594 correct_num: 76.0\n",
      "batch_size: 128 acc: 0.477 correct_num: 61.0\n",
      "batch_size: 128 acc: 0.305 correct_num: 39.0\n",
      "batch_size: 128 acc: 0.469 correct_num: 60.0\n",
      "batch_size: 128 acc: 0.430 correct_num: 55.0\n",
      "batch_size: 128 acc: 0.617 correct_num: 79.0\n",
      "batch_size: 128 acc: 0.484 correct_num: 62.0\n",
      "batch_size: 128 acc: 0.523 correct_num: 67.0\n",
      "batch_size: 128 acc: 0.305 correct_num: 39.0\n",
      "batch_size: 128 acc: 0.602 correct_num: 77.0\n",
      "batch_size: 128 acc: 0.484 correct_num: 62.0\n",
      "batch_size: 128 acc: 0.648 correct_num: 83.0\n",
      "batch_size: 128 acc: 0.703 correct_num: 90.0\n",
      "batch_size: 128 acc: 0.562 correct_num: 72.0\n",
      "batch_size: 128 acc: 0.422 correct_num: 54.0\n",
      "batch_size: 128 acc: 0.711 correct_num: 91.0\n",
      "batch_size: 128 acc: 0.516 correct_num: 66.0\n",
      "batch_size: 128 acc: 0.656 correct_num: 84.0\n",
      "batch_size: 128 acc: 0.758 correct_num: 97.0\n",
      "batch_size: 128 acc: 0.523 correct_num: 67.0\n",
      "batch_size: 128 acc: 0.469 correct_num: 60.0\n",
      "batch_size: 34 acc: 0.853 correct_num: 29.0\n",
      "测试集准确率: 0.556\n",
      "batch_size: 128 acc: 0.961 correct_num: 123.0\n",
      "batch_size: 128 acc: 0.984 correct_num: 126.0\n",
      "batch_size: 128 acc: 0.969 correct_num: 124.0\n",
      "batch_size: 128 acc: 0.961 correct_num: 123.0\n",
      "batch_size: 128 acc: 1.000 correct_num: 128.0\n",
      "batch_size: 128 acc: 0.984 correct_num: 126.0\n",
      "batch_size: 128 acc: 0.992 correct_num: 127.0\n",
      "batch_size: 128 acc: 0.961 correct_num: 123.0\n",
      "batch_size: 128 acc: 0.953 correct_num: 122.0\n",
      "batch_size: 128 acc: 0.992 correct_num: 127.0\n",
      "batch_size: 128 acc: 0.977 correct_num: 125.0\n",
      "batch_size: 128 acc: 0.984 correct_num: 126.0\n",
      "batch_size: 128 acc: 0.984 correct_num: 126.0\n",
      "batch_size: 128 acc: 0.977 correct_num: 125.0\n",
      "batch_size: 128 acc: 0.961 correct_num: 123.0\n",
      "batch_size: 128 acc: 0.977 correct_num: 125.0\n",
      "batch_size: 128 acc: 0.906 correct_num: 116.0\n",
      "batch_size: 128 acc: 1.000 correct_num: 128.0\n",
      "batch_size: 128 acc: 0.984 correct_num: 126.0\n",
      "batch_size: 128 acc: 0.992 correct_num: 127.0\n",
      "batch_size: 128 acc: 0.992 correct_num: 127.0\n",
      "batch_size: 128 acc: 0.984 correct_num: 126.0\n",
      "batch_size: 128 acc: 0.953 correct_num: 122.0\n",
      "batch_size: 128 acc: 0.992 correct_num: 127.0\n",
      "batch_size: 128 acc: 0.969 correct_num: 124.0\n",
      "batch_size: 128 acc: 0.984 correct_num: 126.0\n",
      "batch_size: 128 acc: 0.977 correct_num: 125.0\n",
      "batch_size: 128 acc: 0.922 correct_num: 118.0\n",
      "batch_size: 128 acc: 0.922 correct_num: 118.0\n",
      "batch_size: 128 acc: 0.961 correct_num: 123.0\n",
      "batch_size: 128 acc: 0.938 correct_num: 120.0\n",
      "batch_size: 128 acc: 0.977 correct_num: 125.0\n",
      "batch_size: 128 acc: 0.938 correct_num: 120.0\n",
      "batch_size: 128 acc: 0.969 correct_num: 124.0\n",
      "batch_size: 128 acc: 0.969 correct_num: 124.0\n",
      "batch_size: 128 acc: 0.953 correct_num: 122.0\n",
      "batch_size: 128 acc: 0.977 correct_num: 125.0\n",
      "batch_size: 128 acc: 0.977 correct_num: 125.0\n",
      "batch_size: 128 acc: 0.992 correct_num: 127.0\n",
      "batch_size: 128 acc: 0.969 correct_num: 124.0\n",
      "batch_size: 128 acc: 0.914 correct_num: 117.0\n",
      "batch_size: 128 acc: 0.984 correct_num: 126.0\n",
      "batch_size: 128 acc: 0.969 correct_num: 124.0\n",
      "batch_size: 128 acc: 0.992 correct_num: 127.0\n",
      "batch_size: 128 acc: 0.984 correct_num: 126.0\n",
      "batch_size: 128 acc: 0.961 correct_num: 123.0\n",
      "batch_size: 106 acc: 0.981 correct_num: 104.0\n",
      "训练集准确率: 0.970\n",
      "(5994, 4096) (5794, 4096)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "\n",
    "\n",
    "#用于保存微调后的检查点文件和日志文件路径\n",
    "train_log_dir = './vgg_16_2016_08_28/slim_fine_tune'\n",
    "train_log_file = 'birds_fine_tune.ckpt'\n",
    "\n",
    "\n",
    "assert(tf.gfile.Exists(train_log_dir) == True)\n",
    "\n",
    "#创建一个图，作为当前图\n",
    "with tf.Graph().as_default():\n",
    "\n",
    "    #加载数据\n",
    "    image_holder = tf.placeholder(tf.float32, [None, IMAGE_SIZE, IMAGE_SIZE, 3])\n",
    "    label_holder = tf.placeholder(tf.int32, [None,]) \n",
    "    is_training = tf.placeholder(dtype = tf.bool)\n",
    "\n",
    "    #创建vgg16网络  如果想冻结所有层，可以指定slim.conv2d中的 trainable=False\n",
    "    logits,end_points =  vgg.vgg_16(image_holder, is_training=is_training, num_classes = num_classes)  \n",
    "    \n",
    "    for i in end_points:\n",
    "        print(i)\n",
    "\n",
    "\n",
    "    #预测标签\n",
    "    pred = tf.cast(tf.argmax(logits,axis=1),tf.int32)\n",
    "\n",
    "    #预测结果评估        \n",
    "    correct = tf.equal(pred, label_holder)                    #返回一个数组 表示统计预测正确或者错误 \n",
    "    accuracy = tf.reduce_mean(tf.cast(correct,tf.float32))                #求准确率\n",
    "\n",
    "    num_batch = x_test.shape[0] // batch_size\n",
    "\n",
    "    #用于保存检查点文件 \n",
    "    save = tf.train.Saver(max_to_keep=1) \n",
    "\n",
    "    #恢复模型\n",
    "    with tf.Session() as sess:      \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        #检查最近的检查点文件\n",
    "        ckpt = tf.train.latest_checkpoint(train_log_dir)\n",
    "        if ckpt != None:\n",
    "            save.restore(sess,ckpt)\n",
    "            print('加载上次训练保存后的模型！')\n",
    "        else:\n",
    "            assert(False)\n",
    "                \n",
    "\n",
    "        total_correct = 0.0 \n",
    "        feature_vecs_test = []\n",
    "        for image_batch, label_batch in batch_generator([x_test,y_test], batch_size):                                                            \n",
    "            accuracy_value,layer_outputs = sess.run([accuracy, end_points],feed_dict = {image_holder:image_batch,label_holder:label_batch,is_training:False})\n",
    "            correct_num = image_batch.shape[0]*accuracy_value\n",
    "            print('batch_size: {:2d} acc: {:.3f} correct_num: {:.1f}'.format(image_batch.shape[0],accuracy_value,correct_num))\n",
    "            total_correct += correct_num\n",
    "            \n",
    "            feature = layer_outputs['vgg_16/fc7'].squeeze()\n",
    "            feature_vecs_test.append(feature)\n",
    "        feature_vecs_test = np.vstack(feature_vecs_test)\n",
    "\n",
    "        print('测试集准确率: {:.3f}'.format(total_correct/x_test.shape[0]))\n",
    "        \n",
    "        total_correct = 0.0 \n",
    "        feature_vecs_train = []\n",
    "        for image_batch, label_batch in batch_generator([x_train,y_train], batch_size):                                                            \n",
    "            accuracy_value,layer_outputs = sess.run([accuracy, end_points],feed_dict = {image_holder:image_batch,label_holder:label_batch,is_training:False})\n",
    "            correct_num = image_batch.shape[0]*accuracy_value\n",
    "            print('batch_size: {:2d} acc: {:.3f} correct_num: {:.1f}'.format(image_batch.shape[0],accuracy_value,correct_num))\n",
    "            total_correct += correct_num\n",
    "            \n",
    "            feature = layer_outputs['vgg_16/fc7'].squeeze()\n",
    "            feature_vecs_train.append(feature)\n",
    "        feature_vecs_train = np.vstack(feature_vecs_train)\n",
    "\n",
    "        print('训练集准确率: {:.3f}'.format(total_correct/x_train.shape[0]))\n",
    "        \n",
    "\n",
    "print(feature_vecs_train.shape,feature_vecs_test.shape)\n",
    "\n",
    "def normalize(a):\n",
    "    return np.diag(1/np.sqrt(np.sum(np.square(a),axis=1))).dot(a) #归一化\n",
    "\n",
    "feature_vecs_train, feature_vecs_test = normalize(feature_vecs_train),normalize(feature_vecs_test)\n",
    "\n",
    "np.savez('vgg_feature_vecs',train=feature_vecs_train,test=feature_vecs_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算 mAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mAP @ 1: 0.580\n",
      "time used: 111.193s\n",
      "mAP @ 5: 0.629\n",
      "time used: 113.267s\n",
      "mAP @ 10: 0.613\n",
      "time used: 117.158s\n",
      "mAP @ 50: 0.525\n",
      "time used: 121.564s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 将训练集作为 检索库\n",
    "x_database,y_database = x_train, y_train\n",
    "feature_vecs_database = feature_vecs_train\n",
    "\n",
    "\n",
    "# 从 v_set 中找出和 v 最相似的 k 个元素\n",
    "def topK(v, v_set, k):\n",
    "    dist = np.array([distance(v,i) for i in v_set])\n",
    "    idx = np.argpartition(dist, k)[:k]\n",
    "#   return dist.argsort()[:k][::-1]\n",
    "    return idx[np.argsort(dist[idx])]\n",
    "\n",
    "def distance(a,b):\n",
    "    return -a.dot(b) # 余弦距离\n",
    "    return np.sum(np.square(a-b)) # 欧式距离\n",
    "\n",
    "def onehot2class(onehot):\n",
    "    return np.argmax(onehot) #0-199\n",
    "\n",
    "def retrieve(x,k):\n",
    "    featone = feature_model.predict(np.expand_dims(x, axis=0))\n",
    "    return topK(featone[0],feature_vecs_database,k)\n",
    "\n",
    "def AP(idxK,y):\n",
    "    bool_list = np.array([yi for yi in y_database[idxK]]) == y\n",
    "#     M = num_per_class[y]\n",
    "    M = np.sum(bool_list)\n",
    "    if M == 0:\n",
    "        return 0\n",
    "    return np.sum(np.add.accumulate(bool_list)*bool_list.astype(int)/\n",
    "                  (np.array([i+1 for i in range(len(bool_list))])))/M\n",
    "\n",
    "def mAP(k):\n",
    "    feature_vecs = feature_vecs_test\n",
    "    idxKs = [topK(featone,feature_vecs_database,k) for featone in feature_vecs]\n",
    "    return np.average([ AP(idxK,y) for idxK,y in zip(idxKs,y_test)])\n",
    "\n",
    "import time\n",
    "\n",
    "for k in [1,5,10,50]:\n",
    "    beg = time.time()\n",
    "    print(\"mAP @ {}: {:.3f}\".format(k,mAP(k)))\n",
    "    end = time.time()\n",
    "    print('time used: {:.3f}s'.format(end-beg))\n",
    "\n",
    "# print(feature_model.predict(np.expand_dims(x_test[32], axis=0))) # 稀疏的特征"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
