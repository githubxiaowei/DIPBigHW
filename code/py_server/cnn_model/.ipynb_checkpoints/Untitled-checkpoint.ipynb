{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "自定义 log 函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOGINFO = 1\n",
    "\n",
    "def log_info(*args):\n",
    "    if LOGINFO:\n",
    "        for i in args:\n",
    "            print(i,end=' ')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "导入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def input_data(npz=True):\n",
    "    if npz:\n",
    "        bird_data = np.load('bird_data.npz')\n",
    "        return bird_data['train_img'],bird_data['test_img'],bird_data['train_label'],bird_data['test_label']\n",
    "    else:      \n",
    "        data_path = os.path.join('..','data','CUB_200_2011')\n",
    "        log_info(os.listdir(data_path))\n",
    "\n",
    "        train_test_split_file = os.path.join(data_path,'train_test_split.txt')\n",
    "        with open(train_test_split_file,'r') as file:\n",
    "            train_test_split = np.array([i.split()[1] for i in file.readlines()]).astype('bool')\n",
    "        log_info(train_test_split,train_test_split.size)\n",
    "\n",
    "        img_paths_file = os.path.join(data_path,'images.txt')\n",
    "        with open(img_paths_file,'r') as file:\n",
    "            img_paths = [i.split()[1] for i in file.readlines()]\n",
    "        log_info(img_paths[:1],len(img_paths))\n",
    "\n",
    "        img_labels_file = os.path.join(data_path,'image_class_labels.txt')\n",
    "        with open(img_labels_file,'r') as file:\n",
    "            img_labels = np.array([i.split()[1] for i in file.readlines()]).astype('int')\n",
    "        log_info(img_labels,len(img_labels))\n",
    "\n",
    "        img_dir = os.path.join(data_path,'images')\n",
    "\n",
    "        img_paths_train = [os.path.join(img_dir,os.path.sep.join(path.split('/'))) for i,path in enumerate(img_paths) if train_test_split[i]]\n",
    "        log_info(img_paths_train[:1],len(img_paths_train))\n",
    "        img_paths_test = [os.path.join(img_dir,os.path.sep.join(path.split('/'))) for i,path in enumerate(img_paths) if not train_test_split[i]]\n",
    "        log_info(img_paths_test[:1],len(img_paths_test))\n",
    "\n",
    "        train_img = np.array([cv2.resize(cv2.imread(i),(64,64)) for i in img_paths_train])\n",
    "        test_img = np.array([cv2.resize(cv2.imread(i),(64,64)) for i in img_paths_test])\n",
    "        train_label = np.array([l for i,l in enumerate(img_labels) if train_test_split[i] ])\n",
    "        test_label = np.array([l for i,l in enumerate(img_labels) if not train_test_split[i]])\n",
    "        log_info(train_label,train_label.size)\n",
    "        log_info(test_label,test_label.size)\n",
    "\n",
    "        np.savez('bird_data',train_img=train_img,test_img=test_img,train_label=train_label,test_label=test_label)\n",
    "        return train_img,test_img,train_label,test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type: <class 'numpy.ndarray'> <class 'numpy.ndarray'> \n",
      "shape: (5994, 64, 64, 3) (5994,) \n",
      "size: 73654272 5994 \n"
     ]
    }
   ],
   "source": [
    "x_train,x_test,y_train,y_test = input_data(1)\n",
    "log_info('type:',type(x_train),type(y_train))\n",
    "log_info('shape:',x_train.shape,y_train.shape)\n",
    "log_info('size:',x_train.size,y_train.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据预处理，打乱并拆分 *训练集* 和 *验证集*（5000：994）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type: <class 'numpy.ndarray'> <class 'numpy.ndarray'> \n",
      "shape: (5000, 64, 64, 3) (5000, 200) \n",
      "size: 61440000 1000000 \n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "num_classes = 200\n",
    "\n",
    "# 数据预处理，把 0-255的灰度值转成 0-1 之间的浮点数\n",
    "x_train = x_train.astype('float32')/255\n",
    "x_test = x_test.astype('float32')/255\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train-1, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test-1, num_classes)\n",
    "\n",
    "# shuffle\n",
    "x_train, y_train = np.array(x_train),np.array(y_train)\n",
    "index = [i for i in range(len(y_train))]\n",
    "np.random.shuffle(index)\n",
    "x_train = x_train[index]\n",
    "y_train = y_train[index]\n",
    "\n",
    "# 拆分验证集\n",
    "(x_valid, x_train) = x_train[5000:], x_train[:5000] # 994+5000\n",
    "(y_valid, y_train) = y_train[5000:], y_train[:5000]\n",
    "\n",
    "log_info('type:',type(x_train),type(y_train))\n",
    "log_info('shape:',x_train.shape,y_train.shape)\n",
    "log_info('size:',x_train.size,y_train.size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据扩充"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    " \n",
    "datagen_train = ImageDataGenerator(\n",
    "    width_shift_range = 0.1,\n",
    "    height_shift_range = 0.1,\n",
    "    rotation_range=30,    #0-180\n",
    "    horizontal_flip = True)\n",
    "\n",
    "datagen_train.fit(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CNN**: 3个卷积层+2个全连接层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 8, 8, 128)         73856     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "feat_vec (Dense)             (None, 512)               1049088   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 200)               102600    \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 200)               0         \n",
      "=================================================================\n",
      "Total params: 1,244,936\n",
      "Trainable params: 1,244,936\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), padding='same', strides=2, input_shape=x_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(512, activation='relu', name='feat_vec'))\n",
    "\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# initiate optimizer\n",
    "sgd = keras.optimizers.SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "# train the model using RMSprop\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "weights_path = 'bird.best.hdf5'\n",
    "# model.load_weights(weights_path)\n",
    "# batch_size=64\n",
    "# checkpoint = ModelCheckpoint(filepath=weights_path, verbose=2, save_best_only=True)\n",
    "# history = model.fit_generator(datagen_train.flow(x_train, y_train, batch_size=batch_size),\n",
    "#                 steps_per_epoch=x_train.shape[0] // batch_size,\n",
    "#                 epochs = 500,\n",
    "#                 shuffle=True,\n",
    "#                 verbose=1,\n",
    "#                 callbacks=[checkpoint],\n",
    "#                 validation_data=(x_valid, y_valid),\n",
    "#                 validation_steps=x_valid.shape[0] // batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试集上分类准确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5794/5794 [==============================] - 4s 659us/step\n",
      "evaluate: loss:4.785721931862428 acc:0.15516051087331723\n"
     ]
    }
   ],
   "source": [
    "model.load_weights(weights_path)\n",
    "# evaluate\n",
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print('evaluate: loss:{} acc:{}'.format(loss, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算数据集特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test = input_data(1)\n",
    "# 数据预处理，把 0-255的灰度值转成 0-1 之间的浮点数\n",
    "x_train = x_train.astype('float32')/255\n",
    "x_test = x_test.astype('float32')/255\n",
    "y_train, y_test = y_train-1, y_test-1 \n",
    "\n",
    "feature_model = Model(inputs=model.input, outputs=model.get_layer('feat_vec').output)\n",
    "feature_vecs_train = feature_model.predict(x_train)\n",
    "feature_vecs_test = feature_model.predict(x_test)\n",
    "np.savez('feature_vecs',train=feature_vecs_train,test=feature_vecs_test)\n",
    "# feature_vecs_test_01 = (feature_vecs_test>0).astype(int)\n",
    "# feature_vecs_train_01 = (feature_vecs_train>0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mAP @ 1: 0.00356333742000246\n",
      "time used: 37.88679480552673s\n",
      "mAP @ 5: 0.009109373946095643\n",
      "time used: 35.294941902160645s\n",
      "mAP @ 10: 0.012631095861518534\n",
      "time used: 35.273383140563965s\n",
      "mAP @ 50: 0.022817139255647614\n",
      "time used: 34.55383896827698s\n"
     ]
    }
   ],
   "source": [
    "num_per_class = [np.sum(np.array(y_train) == i) for i in range(num_classes)] # 每一类鸟的训练样本数量 29-30\n",
    "\n",
    "def normalize(a):\n",
    "    return np.diag(1/np.sqrt(np.sum(np.square(a),axis=1))).dot(a) #归一化\n",
    "feature_vecs_train, feature_vecs_test = normalize(feature_vecs_train),normalize(feature_vecs_test)\n",
    "\n",
    "# 将训练集作为 检索库\n",
    "x_database,y_database = x_train, y_train\n",
    "feature_vecs_database = feature_vecs_train\n",
    "\n",
    "\n",
    "# 从 v_set 中找出和 v 最相似的 k 个元素\n",
    "def topK(v, v_set, k):\n",
    "    dist = np.array([distance(v,i) for i in v_set])\n",
    "    idx = np.argpartition(dist, k)[:k]\n",
    "#   return dist.argsort()[:k][::-1]\n",
    "    return idx[np.argsort(dist[idx])]\n",
    "\n",
    "def distance(a,b):\n",
    "    return -a.dot(b) # 余弦距离\n",
    "    return np.sum(np.square(a-b)) # 欧式距离\n",
    "\n",
    "def onehot2class(onehot):\n",
    "    return np.argmax(onehot) #0-199\n",
    "\n",
    "def retrieve(x,k):\n",
    "    featone = feature_model.predict(np.expand_dims(x, axis=0))\n",
    "    return topK(featone,feature_vecs_database,k)\n",
    "\n",
    "def AP(idxK,y):\n",
    "    bool_list = np.array([yi for yi in y_database[idxK]]) == y\n",
    "    M = num_per_class[y]\n",
    "    return np.sum(np.add.accumulate(bool_list)*bool_list.astype(int)/\n",
    "                  (np.array([i+1 for i in range(len(bool_list))])))/M\n",
    "\n",
    "def mAP(x_batch,y_batch,k):\n",
    "    feature_vecs = feature_model.predict(x_batch)\n",
    "    idxKs = [topK(featone,feature_vecs_database,k) for featone in feature_vecs]\n",
    "    return np.average([ AP(idxK,y) for idxK,y in zip(idxKs,y_batch)])\n",
    "\n",
    "import time\n",
    "\n",
    "for k in [1,5,10,50]:\n",
    "    beg = time.time()\n",
    "    print(\"mAP @ {}: {}\".format(k,mAP(x_test,y_test,k)))\n",
    "    end = time.time()\n",
    "    print('time used: {}s'.format(end-beg))\n",
    "\n",
    "# print(feature_model.predict(np.expand_dims(x_test[32], axis=0))) # 稀疏的特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in x_train:\n",
    "    cv2.imshow('img',i)\n",
    "    cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
