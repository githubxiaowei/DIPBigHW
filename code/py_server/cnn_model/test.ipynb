{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "自定义 log 函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOGINFO = 1\n",
    "\n",
    "def log_info(*args):\n",
    "    if LOGINFO:\n",
    "        for i in args:\n",
    "            print(i,end=' ')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "导入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "def input_data(npz=True):\n",
    "    if os.path.exists('bird_data.npz') :\n",
    "        bird_data = np.load('bird_data.npz')\n",
    "        return bird_data['train_img'],bird_data['test_img'],bird_data['train_label'],bird_data['test_label']\n",
    "    else:      \n",
    "        data_path = os.path.join('..','data','CUB_200_2011')\n",
    "        log_info(os.listdir(data_path))\n",
    "\n",
    "        train_test_split_file = os.path.join(data_path,'train_test_split.txt')\n",
    "        with open(train_test_split_file,'r') as file:\n",
    "            train_test_split = np.array([i.split()[1] for i in file.readlines()]).astype('bool')\n",
    "        log_info(train_test_split,train_test_split.size)\n",
    "\n",
    "        img_paths_file = os.path.join(data_path,'images.txt')\n",
    "        with open(img_paths_file,'r') as file:\n",
    "            img_paths = [i.split()[1] for i in file.readlines()]\n",
    "        log_info(img_paths[:1],len(img_paths))\n",
    "\n",
    "        img_labels_file = os.path.join(data_path,'image_class_labels.txt')\n",
    "        with open(img_labels_file,'r') as file:\n",
    "            img_labels = np.array([i.split()[1] for i in file.readlines()]).astype('int')\n",
    "        log_info(img_labels,len(img_labels))\n",
    "\n",
    "        img_dir = os.path.join(data_path,'images')\n",
    "\n",
    "        img_paths_train = [os.path.join(img_dir,os.path.sep.join(path.split('/'))) for i,path in enumerate(img_paths) if train_test_split[i]]\n",
    "        log_info(img_paths_train[:1],len(img_paths_train))\n",
    "        img_paths_test = [os.path.join(img_dir,os.path.sep.join(path.split('/'))) for i,path in enumerate(img_paths) if not train_test_split[i]]\n",
    "        log_info(img_paths_test[:1],len(img_paths_test))\n",
    "\n",
    "        train_img = np.array([cv2.resize(cv2.imread(i),(64,64)) for i in img_paths_train])\n",
    "        test_img = np.array([cv2.resize(cv2.imread(i),(64,64)) for i in img_paths_test])\n",
    "        train_label = np.array([l for i,l in enumerate(img_labels) if train_test_split[i] ])\n",
    "        test_label = np.array([l for i,l in enumerate(img_labels) if not train_test_split[i]])\n",
    "        log_info(train_label,train_label.size)\n",
    "        log_info(test_label,test_label.size)\n",
    "\n",
    "        np.savez('bird_data',train_img=train_img,test_img=test_img,train_label=train_label,test_label=test_label)\n",
    "        return train_img,test_img,train_label,test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test = input_data()\n",
    "log_info('type:',type(x_train),type(y_train))\n",
    "log_info('shape:',x_train.shape,y_train.shape)\n",
    "log_info('size:',x_train.size,y_train.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据预处理，打乱并拆分 *训练集* 和 *验证集*（5000：994）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import keras\n",
    "\n",
    "num_classes = 200\n",
    "\n",
    "# 数据预处理，把 0-255的灰度值转成 0-1 之间的浮点数\n",
    "x_train = x_train.astype('float32')/255\n",
    "x_test = x_test.astype('float32')/255\n",
    "y_train, y_test = np.array(y_train)-1, np.array(y_test)-1\n",
    "# Convert class vectors to binary class matrices.\n",
    "# y_train = keras.utils.to_categorical(y_train-1, num_classes)\n",
    "# y_test = keras.utils.to_categorical(y_test-1, num_classes)\n",
    "\n",
    "# shuffle\n",
    "x_train, y_train = np.array(x_train),np.array(y_train)\n",
    "index = [i for i in range(len(y_train))]\n",
    "np.random.shuffle(index)\n",
    "x_train = x_train[index]\n",
    "y_train = y_train[index]\n",
    "\n",
    "# # 拆分验证集\n",
    "# (x_valid, x_train) = x_train[5000:], x_train[:5000] # 994+5000\n",
    "# (y_valid, y_train) = y_train[5000:], y_train[:5000]\n",
    "\n",
    "log_info('type:',type(x_train),type(y_train))\n",
    "log_info('shape:',x_train.shape,y_train.shape)\n",
    "log_info('size:',x_train.size,y_train.size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "向量外积"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def outer_product(a, b):\n",
    "    OP_DIM = a.shape[1]\n",
    "\n",
    "    template1 = np.zeros([OP_DIM,OP_DIM*OP_DIM])\n",
    "    for i in range(OP_DIM):\n",
    "        for j in range(OP_DIM):\n",
    "            template1[i,OP_DIM*i+j] = 1\n",
    "\n",
    "    ''' \n",
    "    OP_DIM = 4\n",
    "    [[1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
    "     [0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
    "     [0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0.]\n",
    "     [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1.]] \n",
    "\n",
    "    '''\n",
    "    template2 = np.zeros([OP_DIM,OP_DIM*OP_DIM])\n",
    "    for i in range(OP_DIM):\n",
    "        for j in range(OP_DIM):\n",
    "            template2[i,OP_DIM*j+i] = 1\n",
    "\n",
    "    '''\n",
    "    OP_DIM = 4\n",
    "    [[1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0.]\n",
    "     [0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0.]\n",
    "     [0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0.]\n",
    "     [0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1.]] \n",
    "    '''\n",
    "    tml1 = tf.convert_to_tensor(template1, dtype=float)\n",
    "    tml2 = tf.convert_to_tensor(template2, dtype=float)\n",
    "    return tf.matmul(a,tml1)*tf.matmul(b,tml2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "batch generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_aligned_list(data):\n",
    "    \"\"\"Shuffle arrays in a list by shuffling each array identically.\"\"\"\n",
    "    num = data[0].shape[0]\n",
    "    p = np.random.permutation(num)\n",
    "    return [d[p] for d in data]\n",
    "\n",
    "def batch_generator(data, batch_size, shuffle=True):\n",
    "    \"\"\"Generate batches of data.\n",
    "\n",
    "    Given a list of array-like objects, generate batches of a given\n",
    "    size by yielding a list of array-like objects corresponding to the\n",
    "    same slice of each input.\n",
    "    \"\"\"\n",
    "    if shuffle:\n",
    "        data = shuffle_aligned_list(data)\n",
    "\n",
    "    batch_count = 0\n",
    "    while True:\n",
    "        if batch_count * batch_size + batch_size > len(data[0]):\n",
    "            batch_count = 0\n",
    "\n",
    "            if shuffle:\n",
    "                data = shuffle_aligned_list(data)\n",
    "\n",
    "        start = batch_count * batch_size\n",
    "        end = start + batch_size\n",
    "        batch_count += 1\n",
    "        yield [d[start:end] for d in data]\n",
    "\n",
    "data_gen = batch_generator([x_test,y_test], batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def _variable_with_weight_loss(name, shape, stddev, wl):\n",
    "    var = _variable_on_gpu(name, shape,\n",
    "                         tf.truncated_normal_initializer(stddev=stddev))\n",
    "    if wl is not None:\n",
    "        weight_decay = tf.multiply(tf.nn.l2_loss(var), wl, name='weight_loss')\n",
    "        tf.add_to_collection('losses', weight_decay)\n",
    "    return var\n",
    "\n",
    "def _variable_on_gpu(name, shape, initializer):\n",
    "    with tf.device('/gpu:0'):\n",
    "        var = tf.get_variable(name, shape, initializer=initializer)\n",
    "    return var\n",
    "\n",
    "\n",
    "conv_feat_dim = 64\n",
    "\n",
    "\n",
    "image_holder = tf.placeholder(tf.float32, [batch_size, 64, 64, 3])\n",
    "label_holder = tf.placeholder(tf.int32, [batch_size])\n",
    "\n",
    "def conv_net(name):\n",
    "    with tf.variable_scope(name):\n",
    "\n",
    "        with tf.variable_scope('conv1') as scope:\n",
    "            weight1 = _variable_with_weight_loss('weights',shape=[5, 5, 3, 64], stddev=5e-2, wl=0.0)\n",
    "            kernel1 = tf.nn.conv2d(image_holder, weight1, [1, 1, 1, 1], padding='SAME')\n",
    "            bias1 = tf.Variable(tf.constant(0.0, shape=[64]),name='bias')\n",
    "            conv1 = tf.nn.relu(tf.nn.bias_add(kernel1, bias1), name=scope.name)\n",
    "        pool1 = tf.nn.max_pool(conv1, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "        norm1 = tf.nn.lrn(pool1, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75)\n",
    "\n",
    "        with tf.variable_scope('conv2') as scope:\n",
    "            weight2 = _variable_with_weight_loss('weights',shape=[5, 5, 64, 64], stddev=5e-2, wl=0.0)\n",
    "            kernel2 = tf.nn.conv2d(norm1, weight2, [1, 1, 1, 1], padding='SAME')\n",
    "            bias2 = tf.Variable(tf.constant(0.1, shape=[64]),name='bias')\n",
    "            conv2 = tf.nn.relu(tf.nn.bias_add(kernel2, bias2), name=scope.name)\n",
    "        norm2 = tf.nn.lrn(conv2, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75)\n",
    "        pool2 = tf.nn.max_pool(norm2, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "        # 全连接层\n",
    "        with tf.variable_scope('local3') as scope:\n",
    "            reshape = tf.reshape(pool2, [batch_size, -1])\n",
    "            dim = reshape.get_shape()[1].value\n",
    "            weight3 = _variable_with_weight_loss('weights',shape=[dim, 384], stddev=0.04, wl=0.004)\n",
    "            bias3 = tf.Variable(tf.constant(0.1, shape=[384]),name='bias')\n",
    "            local3 = tf.nn.relu(tf.matmul(reshape, weight3) + bias3, name=scope.name)\n",
    "\n",
    "        # 全连接层\n",
    "        with tf.variable_scope('local4') as scope:\n",
    "            weight4 = _variable_with_weight_loss('weights',shape=[384, conv_feat_dim], stddev=0.04, wl=0.004)\n",
    "            bias4 = tf.Variable(tf.constant(0.1, shape=[conv_feat_dim]),name='bias')\n",
    "            local4 = tf.nn.relu(tf.matmul(local3, weight4) + bias4, name=scope.name)\n",
    "\n",
    "    return local4\n",
    "\n",
    "part1 = conv_net('conv_net1')\n",
    "part2 = conv_net('conv_net2')\n",
    "product = outer_product(part1, part2)\n",
    "\n",
    "\n",
    "with tf.variable_scope('softmax_linear') as scope:\n",
    "    weight5 = _variable_with_weight_loss('weights',shape=[product.shape[1], num_classes], stddev=1 / conv_feat_dim, wl=0.0)\n",
    "    bias5 = tf.Variable(tf.constant(0.0, shape=[num_classes]),name='bias')\n",
    "    logits = tf.add(tf.matmul(product, weight5), bias5, name=scope.name)\n",
    "\n",
    "\n",
    "def loss(logits, labels):\n",
    "\n",
    "    labels = tf.cast(labels, tf.int64)\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        logits=logits, labels=labels, name='cross_entropy_per_example')\n",
    "\n",
    "    cross_entropy_mean = tf.reduce_mean(cross_entropy,\n",
    "                                        name='cross_entropy')\n",
    "\n",
    "    tf.add_to_collection('losses', cross_entropy_mean)\n",
    "\n",
    "    return tf.add_n(tf.get_collection('losses'), name='total_loss')\n",
    "\n",
    "\n",
    "loss = loss(logits, label_holder)\n",
    "\n",
    "global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "\n",
    "starter_learning_rate = 0.001\n",
    "learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "                                           5000, 0.96, staircase=True)\n",
    "\n",
    "train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "# train_op = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "top_k_op = tf.nn.in_top_k(logits, label_holder, 1)\n",
    "\n",
    "\n",
    "saver = tf.train.Saver(tf.global_variables(),max_to_keep=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    " \n",
    "datagen_train = ImageDataGenerator(\n",
    "    width_shift_range = 0.1,\n",
    "    height_shift_range = 0.1,\n",
    "    rotation_range=30,    #0-180\n",
    "    horizontal_flip = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "max_steps = 1000000\n",
    "model_dir = 'model'\n",
    "restored_global_step = 0\n",
    "\n",
    "# sess = tf.InteractiveSession()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    ckpt = tf.train.get_checkpoint_state(model_dir)  # 注意此处是checkpoint存在的目录\n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "        saver.restore(sess,ckpt.model_checkpoint_path) # 自动恢复model_checkpoint_path保存模型,一般是最新\n",
    "        print(\"Model restored ...\")\n",
    "        restored_global_step = global_step.eval()\n",
    "        print('restored_global_step: ',restored_global_step)\n",
    "    else:\n",
    "        print('Start from scratch ...')\n",
    "\n",
    "\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        \n",
    "        image_batch, label_batch = next(datagen_train.flow(x_train, y_train, batch_size=batch_size))\n",
    "#         print(label_batch)\n",
    "#         for i in image_batch:\n",
    "#             cv2.imshow('pic',i)\n",
    "#             cv2.waitKey(0)\n",
    "        \n",
    "        if step % 100 == 0 or (step + 1) == max_steps:\n",
    "            \n",
    "            preditcions, loss_value = sess.run([top_k_op, loss], feed_dict={image_holder: image_batch,label_holder: label_batch})\n",
    "            print('train batch precision: {} loss: {}'.format(np.sum(preditcions)/batch_size, loss_value))\n",
    "            \n",
    "            test_image_batch, test_label_batch = next(data_gen)\n",
    "            preditcions, loss_value = sess.run([top_k_op, loss], feed_dict={image_holder: test_image_batch,label_holder: test_label_batch})\n",
    "            print('test  batch precision: {} loss: {}'.format(np.sum(preditcions)/batch_size, loss_value))\n",
    "\n",
    "            \n",
    "            checkpoint_path = os.path.join(model_dir, 'model.ckpt')\n",
    "            g_step = restored_global_step+step\n",
    "            sess.run(tf.assign(global_step,g_step))\n",
    "            saver.save(sess, checkpoint_path, global_step=g_step)\n",
    "            \n",
    "            continue\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        _, loss_value = sess.run([train_op, loss],\n",
    "                                 feed_dict={image_holder: image_batch, label_holder: label_batch})\n",
    "\n",
    "        duration = time.time() - start_time\n",
    "        if step % 10 == 0:\n",
    "\n",
    "            examples_per_sec = batch_size / duration\n",
    "            sec_per_batch = float(duration)\n",
    "            g_step = restored_global_step+step\n",
    "            format_str = ('global_step: %d, loss: %.2f (%.1f examples/sec; %.3f sec/batch) lr：%g')\n",
    "            print(format_str % (g_step, loss_value, examples_per_sec, sec_per_batch, sess.run(learning_rate)))\n",
    "            \n",
    "\n",
    "        \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试集上分类准确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    ckpt = tf.train.get_checkpoint_state(model_dir)  # 注意此处是checkpoint存在的目录\n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "        saver.restore(sess,ckpt.model_checkpoint_path)\n",
    "#     saver = tf.train.import_meta_graph('model/model.ckpt.meta')\n",
    "#     saver.restore(sess, tf.train.latest_checkpoint(model_dir))\n",
    "    # 样本数\n",
    "    data_gen = batch_generator([x_test,y_test], batch_size, shuffle=False)\n",
    "    num_examples = x_test.shape[0]\n",
    "    num_iter = num_examples // batch_size\n",
    "    total_sample_count = num_iter * batch_size\n",
    "\n",
    "    step = 0\n",
    "    true_count = 0\n",
    "    while step < num_iter:\n",
    "\n",
    "        image_batch, label_batch = next(data_gen)\n",
    "\n",
    "        preditcions = sess.run([top_k_op], feed_dict={image_holder: image_batch,\n",
    "                                                      label_holder: label_batch})\n",
    "        true_count += np.sum(preditcions)\n",
    "        step += 1\n",
    "\n",
    "precision = true_count / total_sample_count\n",
    "print('precision @ 1 = %.3f' % precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
