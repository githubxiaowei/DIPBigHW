{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考 http://www.cnblogs.com/zyly/p/9146787.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "224\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.slim.nets import vgg\n",
    "\n",
    "IMAGE_SIZE = vgg.vgg_16.default_image_size\n",
    "print(IMAGE_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type: <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      "shape: (5994, 224, 224, 3) (5994,)\n",
      "size: 902264832 5994\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-8af00d3d0240>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;31m# 数据预处理，把 0-255的灰度值转成 0-1 之间的浮点数\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m \u001b[0mx_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'float32'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m255\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m \u001b[0mx_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'float32'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m255\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "def input_data(npz_file):\n",
    "    if os.path.exists(npz_file) :\n",
    "        bird_data = np.load(npz_file)\n",
    "        return bird_data['train_img'],bird_data['test_img'],bird_data['train_label'],bird_data['test_label']\n",
    "    else:      \n",
    "        data_path = os.path.join('../../..','data','CUB_200_2011')\n",
    "        print(os.listdir(data_path))\n",
    "\n",
    "        train_test_split_file = os.path.join(data_path,'train_test_split.txt')\n",
    "        with open(train_test_split_file,'r') as file:\n",
    "            train_test_split = np.array([i.split()[1] for i in file.readlines()]).astype('bool')\n",
    "        print(train_test_split,train_test_split.size)\n",
    "\n",
    "        img_paths_file = os.path.join(data_path,'images.txt')\n",
    "        with open(img_paths_file,'r') as file:\n",
    "            img_paths = [i.split()[1] for i in file.readlines()]\n",
    "        print(img_paths[:1],len(img_paths))\n",
    "\n",
    "        img_labels_file = os.path.join(data_path,'image_class_labels.txt')\n",
    "        with open(img_labels_file,'r') as file:\n",
    "            img_labels = np.array([i.split()[1] for i in file.readlines()]).astype('int')\n",
    "        print(img_labels,len(img_labels))\n",
    "\n",
    "        img_dir = os.path.join(data_path,'images')\n",
    "\n",
    "        img_paths_train = [os.path.join(img_dir,os.path.sep.join(path.split('/'))) for i,path in enumerate(img_paths) if train_test_split[i]]\n",
    "        print(img_paths_train[:1],len(img_paths_train))\n",
    "        img_paths_test = [os.path.join(img_dir,os.path.sep.join(path.split('/'))) for i,path in enumerate(img_paths) if not train_test_split[i]]\n",
    "        print(img_paths_test[:1],len(img_paths_test))\n",
    "\n",
    "        train_img = np.array([cv2.resize(cv2.imread(i),(224,224)) for i in img_paths_train])\n",
    "        test_img = np.array([cv2.resize(cv2.imread(i),(224,224)) for i in img_paths_test])\n",
    "        train_label = np.array([l for i,l in enumerate(img_labels) if train_test_split[i] ])\n",
    "        test_label = np.array([l for i,l in enumerate(img_labels) if not train_test_split[i]])\n",
    "        print(train_label,train_label.size)\n",
    "        print(test_label,test_label.size)\n",
    "\n",
    "        np.savez(npz_file,train_img=train_img,test_img=test_img,train_label=train_label,test_label=test_label)\n",
    "        return train_img,test_img,train_label,test_label\n",
    "    \n",
    "x_train,x_test,y_train,y_test = input_data('bird_data_224.npz')\n",
    "print('type:',type(x_train),type(y_train))\n",
    "print('shape:',x_train.shape,y_train.shape)\n",
    "print('size:',x_train.size,y_train.size)\n",
    "\n",
    "\n",
    "num_classes = 200\n",
    "\n",
    "# 数据预处理，把 0-255的灰度值转成 0-1 之间的浮点数\n",
    "x_train = x_train.astype('float32')/255\n",
    "x_test = x_test.astype('float32')/255\n",
    "y_train, y_test = np.array(y_train)-1, np.array(y_test)-1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "演示一个VGG16的例子 \n",
    "微调 这里只调整VGG16最后一层全连接层，把1000类改为5类 \n",
    "对网络进行训练   使用slim库简化代码\n",
    "'''    \n",
    "batch_size = 128\n",
    "\n",
    "learning_rate = 1e-4\n",
    "\n",
    "#用于保存微调后的检查点文件和日志文件路径\n",
    "train_log_dir = './vgg_16_2016_08_28/slim_fine_tune'    \n",
    "\n",
    "#官方下载的检查点文件路径\n",
    "checkpoint_file = './vgg_16_2016_08_28/vgg_16.ckpt'\n",
    "\n",
    "if not tf.gfile.Exists(train_log_dir):\n",
    "    tf.gfile.MakeDirs(train_log_dir)\n",
    "\n",
    "#创建一个图，作为当前图\n",
    "with tf.Graph().as_default():\n",
    "\n",
    "    #加载数据\n",
    "    train_images, train_labels = input_data.get_batch_images_and_label(DATA_DIR,batch_size,NUM_CLASSES,True,IMAGE_SIZE,IMAGE_SIZE)          \n",
    "\n",
    "\n",
    "    #创建vgg16网络  如果想冻结所有层，可以指定slim.conv2d中的 trainable=False\n",
    "    logits,end_points =  vgg.vgg_16(train_images, is_training=True,num_classes = NUM_CLASSES)        \n",
    "\n",
    "    #交叉熵代价函数\n",
    "    slim.losses.softmax_cross_entropy(logits, onehot_labels=train_labels)\n",
    "    total_loss = slim.losses.get_total_loss()\n",
    "\n",
    "    #设置写入到summary中的变量\n",
    "    tf.summary.scalar('losses/total_loss', total_loss)\n",
    "\n",
    "    '''\n",
    "    设置优化器 这里不能指定成Adam优化器，因为我们的官方模型文件中使用的就是GradientDescentOptimizer优化器，\n",
    "    因此我们要和官方模型一致，如果想使用AdamOptimizer优化器，我们可以在调用完vgg16()网络后，就执行恢复模型。\n",
    "    而把执行恢复模型的代码放在后面，会由于我们在当前图中定义了一些检查点中不存在变量，恢复时在检查点文件找不\n",
    "    到变量，因此会报错。\n",
    "    '''\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    #optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    # create_train_op that ensures that when we evaluate it to get the loss,\n",
    "    # the update_ops are done and the gradient updates are computed.\n",
    "    train_tensor = slim.learning.create_train_op(total_loss, optimizer)\n",
    "\n",
    "    #检查最近的检查点文件\n",
    "    ckpt = tf.train.latest_checkpoint(train_log_dir)\n",
    "    if ckpt != None:\n",
    "        variables_to_restore = slim.get_model_variables()\n",
    "        init_fn = slim.assign_from_checkpoint_fn(ckpt,variables_to_restore)\n",
    "        print('从上次训练保存后的模型继续训练！')\n",
    "    else:\n",
    "        # Restore only the convolutional layers: 从检查点载入除了fc8层之外的参数到当前图             \n",
    "        variables_to_restore = slim.get_variables_to_restore(exclude=['vgg_16/fc8']) \n",
    "        init_fn = slim.assign_from_checkpoint_fn(checkpoint_file, variables_to_restore)\n",
    "        print('从官方模型加载训练！')\n",
    "\n",
    "\n",
    "    print('开始训练！')\n",
    "    #开始训练网络        \n",
    "    slim.learning.train(train_tensor,\n",
    "                        train_log_dir,\n",
    "                        number_of_steps=100,             #迭代次数 一次迭代batch_size个样本\n",
    "                        save_summaries_secs=300,         #存summary间隔秒数\n",
    "                        save_interval_secs=300,          #存模模型间隔秒数                         \n",
    "                        init_fn=init_fn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
